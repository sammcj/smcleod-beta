<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>storage on smcleod.net</title><link>https://beta.smcleod.net/tags/storage/</link><description>Recent content in storage on smcleod.net</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><copyright>Sam McLeod</copyright><lastBuildDate>Tue, 20 Mar 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://beta.smcleod.net/tags/storage/index.xml" rel="self" type="application/rss+xml"/><item><title>Flash Storage and SSD Failure Rate Update (March 2018)</title><link>https://beta.smcleod.net/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/</guid><description>&lt;p>It was almost 3 years ago that my open source storage project went into production. In that time it&amp;rsquo;s been running 24/7 serving as highly available solid state storage for hundreds of VMs and several virtualisation clusters across our two main sites.&lt;/p>
&lt;p>I&amp;rsquo;m happy to report that the clusters have been operating very successfully since their conception.&lt;/p>
&lt;p>Since moving away from proprietary &amp;lsquo;black box&amp;rsquo; vendor SANs, we haven&amp;rsquo;t had a single SAN issue, storage outage.&lt;/p>
&lt;p>We&amp;rsquo;ve also been so please with the success of the design that we&amp;rsquo;ve grown from a single cluster at each site to a total of 6 clusters (12 nodes).&lt;/p></description></item><item><title>Talk - Clustered, Distributed File and Volume Storage with GlusterFS</title><link>https://beta.smcleod.net/2017/11/talk-clustered-distributed-file-and-volume-storage-with-glusterfs/</link><pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2017/11/talk-clustered-distributed-file-and-volume-storage-with-glusterfs/</guid><description>&lt;p>Using GlusterFS to provide volume storage to Kubernetes as a replacement for our existing file and static content hosting.&lt;/p>
&lt;p>This talk was given at &lt;a href="https://www.meetup.com/Infrastructure-Coders/events/244535588/">Infracoders&lt;/a> on Tuesday 14th Novemember 2017.&lt;/p>
&lt;p>NOTE: Below link to slides currently broken - will fix soon! (03/08/2019)&lt;/p>
&lt;p>Click below to view slides (PDF version):
&lt;a href="https://www.dropbox.com/s/rdojhb399639e4k/lightning_san.pdf?dl=0">![Click to view slides]({{ site.url }}/img/gluster-first-slide.jpg)&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.dropbox.com/s/rdojhb399639e4k/lightning_san.pdf?dl=1">&lt;em>Direct download link&lt;/em>&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GlusterFS</title><link>https://beta.smcleod.net/2017/09/glusterfs/</link><pubDate>Mon, 25 Sep 2017 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2017/09/glusterfs/</guid><description>&lt;p>We&amp;rsquo;re in the process of shifting from using our custom &amp;lsquo;glue&amp;rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&amp;rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.&lt;/p>
&lt;p>The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.&lt;/p>
&lt;p>I spent time evaluating various cluster filesystems / network block storage and the two that stood out were Ceph and Gluster and settled on Gluster as the most suitable for our needs, it&amp;rsquo;s far less complex to deploy than Ceph, it has less moving pieces and files are stored in a familiar manner on hosts.&lt;/p></description></item><item><title>Update Delayed Serial STONITH Design</title><link>https://beta.smcleod.net/2016/07/update-delayed-serial-stonith-design/</link><pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2016/07/update-delayed-serial-stonith-design/</guid><description>&lt;p>&lt;em>note: This is a follow up post from &lt;a href="https://smcleod.net/tech/2015/07/21/rcd-stonith/">2015-07-21-rcd-stonith&lt;/a>&lt;/em>&lt;/p>
&lt;h3 id="a-linux-cluster-base-stonith-provider-for-use-with-modern-pacemaker-clusters">A Linux Cluster Base STONITH provider for use with modern Pacemaker clusters&lt;/h3>
&lt;p>This has since been accepted and merged into Fedora&amp;rsquo;s code base and as such will make it&amp;rsquo;s way to RHEL.&lt;/p>
&lt;ul>
&lt;li>Source Code: &lt;a href="https://github.com/sammcj/fence_rcd_serial">Github&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://diptrace.com/download/download-diptrace/">Diptrace&lt;/a> CAD Design: &lt;a href="https://github.com/sammcj/fence_rcd_serial/tree/master/CAD/STONTH_CAD_DESIGN_V3">Github&lt;/a>&lt;/li>
&lt;li>I have open sourced the CAD circuit design and made this available within this repo under
&lt;a href="CAD/STONTH_CAD_DESIGN_V3">CAD Design and Schematics&lt;/a>&lt;/li>
&lt;li>Related RedHat Bug: &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1240868">https://bugzilla.redhat.com/show_bug.cgi?id=1240868&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Benchmarking IO with FIO</title><link>https://beta.smcleod.net/2016/04/benchmarking-io-with-fio/</link><pubDate>Fri, 29 Apr 2016 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2016/04/benchmarking-io-with-fio/</guid><description>&lt;h2 id="this-is-a-quick-tldr-there-are-_many_-other-situations-and-options-you-could-consider">This is a quick tldr there are &lt;em>many&lt;/em> other situations and options you could consider&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="http://linux.die.net/man/1/fio">FIO man page&lt;/a>&lt;/li>
&lt;li>IOP/s = Input or Output operations per second&lt;/li>
&lt;li>Throughput = How many MB/s can you read/write continuously&lt;/li>
&lt;/ul></description></item><item><title>Fix XenServer SR with corrupt or invalid metadata</title><link>https://beta.smcleod.net/2016/01/fix-xenserver-sr-with-corrupt-or-invalid-metadata/</link><pubDate>Mon, 18 Jan 2016 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2016/01/fix-xenserver-sr-with-corrupt-or-invalid-metadata/</guid><description>&lt;p>If a disk / VDI is orphaned or only partially deleted you&amp;rsquo;ll notice that under the SR it&amp;rsquo;s not assigned to any VM.&lt;/p>
&lt;p>This can cause issues that look like metadata corruption resulting in the inability to migrate VMs or edit storage.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff79c6">[&lt;/span>root@xenserver-host ~&lt;span style="color:#ff79c6">]&lt;/span>&lt;span style="color:#6272a4"># xe vdi-destroy uuid=6c2cd848-ac0e-441c-9cd6-9865fca7fe8b&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Error code: SR_BACKEND_FAILURE_181
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Error parameters: , Error in Metadata volume operation &lt;span style="color:#ff79c6">for&lt;/span> SR. &lt;span style="color:#ff79c6">[&lt;/span>&lt;span style="color:#8be9fd;font-style:italic">opterr&lt;/span>&lt;span style="color:#ff79c6">=&lt;/span>VDI delete operation failed &lt;span style="color:#ff79c6">for&lt;/span> parameters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> /dev/VG_XenStorage-3ae1df17-06ee-7202-eb92-72c266134e16/MGT, 6c2cd848-ac0e-441c-9cd6-9865fca7fe8b.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Error: Failed to write file with params &lt;span style="color:#ff79c6">[&lt;/span>3, 0, 512, 512&lt;span style="color:#ff79c6">]&lt;/span>. Error: 5&lt;span style="color:#ff79c6">]&lt;/span>,
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>iSCSI SCSI-ID / Serial Persistence</title><link>https://beta.smcleod.net/2015/12/iscsi-scsi-id-/-serial-persistence/</link><pubDate>Mon, 14 Dec 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/12/iscsi-scsi-id-/-serial-persistence/</guid><description>&lt;p>&lt;em>&lt;strong>&amp;ldquo;Having a SCSI ID is a f*cking idiotic thing to do.&amp;rdquo;&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;a href="http://yarchive.net/comp/linux/scsi_ids.html">- Linus Torvalds&lt;/a>&lt;/p>
&lt;p>&lt;em>&amp;hellip;and after the amount of time I&amp;rsquo;ve wasted getting XenServer to play nicely with LIO iSCSI failover I tend to agree.&lt;/em>&lt;/p>
&lt;hr>
&lt;h2 id="the-problem">The Problem&lt;/h2>
&lt;p>![]({{ site.url }}/img/san/sr_fail.jpg)&lt;/p>
&lt;p>One oddity of Xen / XenServer&amp;rsquo;s storage subsystem is that it identifies iSCSI storage repositories via a calculated SCSI ID rather than the iSCSI Serial - which would be the sane thing to do.&lt;/p>
&lt;p>Citrix&amp;rsquo;s less than ideal take on dealing with SCSI ID changes is for you to take your VMs offline, disconnected the storage repositories, recreate them, then go through all your VMs and re-attach their orphaned disks hoping that you remembered to add some sort of hint as to what VM they belong to, then finally wipe the sweat and tears from your face.&lt;/p>
&lt;p>From &lt;a href="https://support.citrix.com/article/CTX118641">CTX11641&lt;/a> - &amp;lsquo;How to Identify If SCSI Storage Repository has Changed SCSI IDs&amp;rsquo;:&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;The SCSI ID of the logical unit number (LUN) changed. When this happened, the iSCSI storage repository became unplugged after a XenServer reboot.&amp;rdquo;
&amp;hellip;
&amp;ldquo;To correct the issue you must recreate a PBD with the entry to reflect the right SCSI ID.&amp;rdquo;&lt;/p>
&lt;/blockquote></description></item><item><title>How to cluster and failover (almost) anything - An intro to Pacemaker and Corosync</title><link>https://beta.smcleod.net/2015/11/how-to-cluster-and-failover-almost-anything-an-intro-to-pacemaker-and-corosync/</link><pubDate>Mon, 09 Nov 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/11/how-to-cluster-and-failover-almost-anything-an-intro-to-pacemaker-and-corosync/</guid><description>&lt;ul>
&lt;li>&lt;strong>Slides:&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://www.dropbox.com/s/n3g3nk9kp6q54h8/cluster_anything.pdf?dl=0">![Click to Start Slides]({{ site.url }}/img/san/cluster_anything_screenshot.jpg)&lt;/a>&lt;/p></description></item><item><title>SAN Intro (Talk)</title><link>https://beta.smcleod.net/2015/10/san-intro-talk/</link><pubDate>Wed, 07 Oct 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/10/san-intro-talk/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>SSD Storage - Two Months In Production</title><link>https://beta.smcleod.net/2015/09/ssd-storage-two-months-in-production/</link><pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/09/ssd-storage-two-months-in-production/</guid><description>&lt;p>Over the last two months I&amp;rsquo;ve been runing selected IO intensive servers off the the SSD storage cluster, these hosts include (among others) our:&lt;/p>
&lt;ul>
&lt;li>Primary Puppetmaster&lt;/li>
&lt;li>Gitlab server&lt;/li>
&lt;li>Redmine app and database servers&lt;/li>
&lt;li>Nagios servers&lt;/li>
&lt;li>Several Docker database host servers&lt;/li>
&lt;/ul></description></item><item><title>iSCSI Benchmarking</title><link>https://beta.smcleod.net/2015/07/iscsi-benchmarking/</link><pubDate>Fri, 24 Jul 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/07/iscsi-benchmarking/</guid><description>&lt;p>The following are benchmarks from our testings of our iSCSI SSD storage.&lt;/p>
&lt;h3 id="67300-read-iops-on-a-vm-on-iscsi">67,300 read IOP/s on a VM on iSCSI&lt;/h3>
&lt;ul>
&lt;li>(Disk -&amp;gt; LVM -&amp;gt; MDADM -&amp;gt; DRBD -&amp;gt; iSCSI target -&amp;gt; Network -&amp;gt; XenServer iSCSI Client -&amp;gt; VM)&lt;/li>
&lt;li>Per VM and scales to 1,000,000 IOP/s total&lt;/li>
&lt;/ul></description></item><item><title>Delayed Serial STONITH</title><link>https://beta.smcleod.net/2015/07/delayed-serial-stonith/</link><pubDate>Tue, 21 Jul 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/07/delayed-serial-stonith/</guid><description>&lt;p>A modified version of &lt;a href="http://www.scl.co.uk/rcd_serial/README.rcd_serial">John Sutton&amp;rsquo;s&lt;/a> rcd_serial cable coupled with our Supermicro reset switch hijacker:&lt;/p>
&lt;p>![]({{ site.url }}/img/san/rcd_serial.jpg)&lt;/p>
&lt;p>This works with the rcd_serial fence agent &lt;a href="https://github.com/ClusterLabs/fence-agents/tree/master/agents/rcd_serial">plugin&lt;/a>.&lt;/p></description></item><item><title>Video - Cluster Failover Performance Demo</title><link>https://beta.smcleod.net/2015/07/video-cluster-failover-performance-demo/</link><pubDate>Sun, 12 Jul 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/07/video-cluster-failover-performance-demo/</guid><description>&lt;!-- raw HTML omitted --></description></item><item><title>CentOS 7 and HA</title><link>https://beta.smcleod.net/2015/07/centos-7-and-ha/</link><pubDate>Tue, 07 Jul 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/07/centos-7-and-ha/</guid><description>&lt;p>First some background&amp;hellip;&lt;/p>
&lt;p>One of the many lessons I&amp;rsquo;ve learnt from my Linux HA / Storage clustering project is that the Debian HA ecosystem is essentially broken, We reached the point where packages were too old, too buggy or in Debian 8&amp;rsquo;s case - outright missing.&lt;/p>
&lt;p>In the past I was very disappointed with RHEL/CentOS 5 / 6 and (until now) have been quite satisfied with Debian as a stable server distribution with historicity more modern packages and kernels.&lt;/p></description></item><item><title>SSD Storage Cluster - Update and Diagram</title><link>https://beta.smcleod.net/2015/06/ssd-storage-cluster-update-and-diagram/</link><pubDate>Wed, 17 Jun 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/06/ssd-storage-cluster-update-and-diagram/</guid><description>&lt;p>Due to several recent events beyond my control I&amp;rsquo;m a bit behind on the project - hence the lack of updates which I appologise for.
The goods news is that I&amp;rsquo;m back working to finish off the clusters and I&amp;rsquo;m happy to report that all is going to plan.&lt;/p>
&lt;p>Here is the final digram of the two-node cluster design:&lt;/p>
&lt;p>![]({{ site.url }}/img/san/diagram.png)&lt;/p>
&lt;p>Plain text version available &lt;a href="https://gist.github.com/sammcj/0503007ceb5038a0de3c">here&lt;/a>&lt;/p></description></item><item><title>Video - Storage Cluster Failover Demo</title><link>https://beta.smcleod.net/2015/05/video-storage-cluster-failover-demo/</link><pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/05/video-storage-cluster-failover-demo/</guid><description>&lt;p>Just whipped up a brief demonstration of the failover and recovery process on the storage clusters I&amp;rsquo;ve built:&lt;/p>
&lt;p>&lt;a href="https://youtu.be/_fRMtXWM3FU">![Click to Start Video]({{ site.url }}/img/san/cluster-demo-thumb.jpg)&lt;/a>&lt;/p></description></item><item><title>Talk - High Performance Software Defined Storage</title><link>https://beta.smcleod.net/2015/04/talk-high-performance-software-defined-storage/</link><pubDate>Wed, 15 Apr 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/04/talk-high-performance-software-defined-storage/</guid><description>&lt;p>A high level talk from Infracoders Melbourne on 12/04/2015.&lt;/p>
&lt;p>&lt;a href="https://www.dropbox.com/s/rdojhb399639e4k/lightning_san.pdf?dl=0">![Click to Start Slides]({{ site.url }}/img/san/supermicrox2.jpg)&lt;/a>&lt;/p></description></item><item><title>Building a high performance SSD SAN - Part 1</title><link>https://beta.smcleod.net/2015/02/building-a-high-performance-ssd-san-part-1/</link><pubDate>Mon, 16 Feb 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/02/building-a-high-performance-ssd-san-part-1/</guid><description>&lt;p>![]({{ site.url }}/img/san/graphs.png)&lt;/p>
&lt;p>Over the coming month I will be architecting, building and testing a modular, high performance SSD-only storage solution.&lt;/p>
&lt;p>I&amp;rsquo;ll be documenting my progress / findings along the way and open sourcing all the information as a public guide.&lt;/p>
&lt;p>With recent price drops and durability improvements in solid state storage now is better time than any to ditch those old magnets.&lt;/p>
&lt;p>Modular server manufacturers such as SuperMicro have spent large on R&amp;amp;D thanks to the ever growing requirements from cloud vendors that utilise their hardware.&lt;/p></description></item><item><title>Direct-Attach SSD Storage - Performance &amp; Comparisons</title><link>https://beta.smcleod.net/2015/02/direct-attach-ssd-storage-performance-comparisons/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/02/direct-attach-ssd-storage-performance-comparisons/</guid><description>Further to my earlier post on XenServer storage performance with regards to directly attaching storage from the host, I have been analysing the performance of various SSD storage options. I have attached a HP DS2220sb storage blade to an existing server blade and compared performance with 4 and 6 SSD RAID-10 to our existing iSCSI SANs.
While the P420i RAID controller in the DS2220sb is clearly saturated and unable to provide throughput much over 1,100MB/s - the IOP/s available to PostgreSQL are still a very considerably performance improvement over our P4530 SAN - in fact, 6 SSD&amp;rsquo;s result in a 39.</description></item><item><title>XenServer, SSDs &amp; VM Storage Performance</title><link>https://beta.smcleod.net/2015/02/xenserver-ssds-vm-storage-performance/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://beta.smcleod.net/2015/02/xenserver-ssds-vm-storage-performance/</guid><description>&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>At Infoxchange we use XenServer as our Virtualisation of choice.
There are many reasons for this including:&lt;/p>
&lt;ul>
&lt;li>Open Source.&lt;/li>
&lt;li>Offers greater performance than VMware.&lt;/li>
&lt;li>Affordability (it&amp;rsquo;s free unless you purchase support).&lt;/li>
&lt;li>Proven backend Xen is very reliable.&lt;/li>
&lt;li>Reliable cross-host migrations of VMs.&lt;/li>
&lt;li>The XenCentre client, (although having to run in a Windows VM) is quick and simple to use.&lt;/li>
&lt;li>Upgrades and patches have proven to be more reliable than VMware.&lt;/li>
&lt;li>OpenStack while interesting, is not yet reliable or streamlined enough for our small team of 4 to implement and manage.&lt;/li>
&lt;li>XenServer Storage &amp;amp; Filesystems&lt;/li>
&lt;/ul></description></item></channel></rss>