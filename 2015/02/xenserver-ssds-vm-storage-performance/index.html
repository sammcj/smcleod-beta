<!doctype html><html lang=en-au><head><meta name=viewport content="width=device-width"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=7"><link rel=icon href=../../../favicon.png><link rel="shortcut icon" href=../../../favicon.ico type=image/x-icon><link rel=apple-touch-icon href=../../../apple-touch-icon.png><link rel=icon href=../../../logo.svg type=image/svg+xml><title>XenServer, SSDs & VM Storage Performance &ndash;
smcleod.net</title><link href=../../../symbols-nerd-font/symbols-nerd-font.css rel=stylesheet><link href=../../../jetbrains-mono/jetbrains-mono.css rel=stylesheet><link type=text/css rel=stylesheet href=https://smcleod.net/css/styles.7b4e25fd623d506ea3f05611f28537bfc348438f566406fa0fb94ce6949036ba7445242995f91836399ef856d7eb7cd634ca793f8db9d576f4ab45557ed4d3fc.css integrity="sha512-e04l/WI9UG6j8FYR8oU3v8NIQ49WZAb6D7lM5pSQNrp0RSQplfkYNjme+FbX63zWNMp5P4251Xb0q0VVftTT/A=="><link href=../../../atkinson-hyperlegible/atkinson-hyperlegible.css rel=stylesheet><meta name=author content="Sam McLeod"><meta name=keywords content="hardware,storage,tech"><meta name=description content="</h2>
<p>At Infoxchange we use XenServer as our Virtualisation of choice.
There are many reasons for this including:</p>
<ul>
<li>Open Source.</li>
<li>Offers greater performance than VMware.</li>
<li>Affordability (it&amp;rsquo;s free unless you purchase support).</li>
<li>Proven backend Xen is very reliable.</li>
<li>Reliable cross-host migrations of VMs.</li>
<li>The XenCentre client, (although having to run in a Windows VM) is quick and simple to use.</li>
<li>Upgrades and patches have proven to be more reliable than VMware.</li>
<li>OpenStack while interesting, is not yet reliable or streamlined enough for our small team of 4 to implement and manage.</li>
<li>XenServer Storage &amp;amp; Filesystems</li>
</ul>"><meta property="og:site_name" content="smcleod.net"><meta property="og:title" content="XenServer, SSDs & VM Storage Performance"><meta property="og:type" content="article"><meta property="article:author" content="Sam McLeod"><meta property="article:published_time" content="2015-02-15T00:00:00Z+0000"><meta property="article:tag" content="hardware"><meta property="article:tag" content="storage"><meta property="article:tag" content="tech"><meta property="og:url" content="https://smcleod.net/2015/02/xenserver-ssds-vm-storage-performance/"><meta property="og:image" content="https://smcleod.net/2015/02/xenserver-ssds-vm-storage-performance//noah-kuhn-27481.jpg"><meta property="og:description" content="<h2 id=&#34;intro&#34;>Intro</h2>
<p>At Infoxchange we use XenServer as our Virtualisation of choice.
There are many reasons for this including:</p>
<ul>
<li>Open Sourc"><meta name=twitter:card content="summary_large_image"><meta property="twitter:domain" content="mcleod.ne"><meta property="twitter:url" content="https://smcleod.net/2015/02/xenserver-ssds-vm-storage-performance/"><meta name=twitter:title content="XenServer, SSDs & VM Storage Performance"><meta name=twitter:image content="https://smcleod.net/2015/02/xenserver-ssds-vm-storage-performance//noah-kuhn-27481.jpg"><meta name=twitter:description content="<h2 id=&#34;intro&#34;>Intro</h2>
<p>At Infoxchange we use XenServer as our Virtualisation of choice.
There are many reasons for this including:</p>
<ul>
<li>Open Sourc"><link rel=manifest href=../../../manifest/index.json></head><body><div id=baseContainer><header><div class=titleAndSearchContainer><div id=titleContainer><a class=unstyledLink href=../../../><img src=../../../logo.svg alt=Logo></a><div class=rightOfLogo><div class=titleAndHamburger><h1><a class=unstyledLink href=../../../>smcleod.net</a></h1></div><div id=wide_nav><nav><ul id=main-nav><li><a href=../../../>Home</a></li><li><a href=../../../posts>Posts</a></li><li><a href=https://smcleod.net/pages/about/>About</a></li><li><a href=https://smcleod.net/pages/links/>Links</a></li><li><a href=../../../tags>Tags</a></li><li><a href=../../../search>Search</a></li></ul></nav></div></div></div><div class=search><input id=searchbar type=text placeholder=Search>
<a class=nerdlink onclick=newSearch()>&#xf002;</a></div><script>function newSearch(){let e=searchbar.value.trim();if(!e)return;location.href=`/search?q=${e}`}searchbar.onkeyup=e=>{e.keyCode==13&&newSearch()}</script></div><div id=links><a rel=noreferrer target=_blank class=nerdlink href=../../../index.xml>&#xf09e;
<span>RSS</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://github.com/sammcj>&#xf09b;
<span>Github</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://twitter.com/s_mcleod>&#xf099;
<span>Twitter</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://linkedin.com/in/sammcj>&#xf0e1;
<span>LinkedIn</span></a></div></header><div id=contentContainer><div id=content><main><article class="card single"><h1>XenServer, SSDs & VM Storage Performance</h1><p class=date><span title=Date></span>
2015-02-15</p><figure style=margin:0><img src=https://smcleod.net/2015/02/xenserver-ssds-vm-storage-performance//noah-kuhn-27481.jpg alt></figure><div><h2 id=intro>Intro</h2><p>At Infoxchange we use XenServer as our Virtualisation of choice.
There are many reasons for this including:</p><ul><li>Open Source.</li><li>Offers greater performance than VMware.</li><li>Affordability (it&rsquo;s free unless you purchase support).</li><li>Proven backend Xen is very reliable.</li><li>Reliable cross-host migrations of VMs.</li><li>The XenCentre client, (although having to run in a Windows VM) is quick and simple to use.</li><li>Upgrades and patches have proven to be more reliable than VMware.</li><li>OpenStack while interesting, is not yet reliable or streamlined enough for our small team of 4 to implement and manage.</li><li>XenServer Storage & Filesystems</li></ul><p>Unfortunately the downside to XenServer is that it&rsquo;s underlying OS is quite old.
The latest version (6.5) about to be released is still based on Centos 5 and still lacks any form of EXT4 and BTRFS support, direct disk access is not available… without some tweaking and has no real support for TRIM unless you have direct disk access and are happy with EXT3.</p><p>Despite this, XenServer still manages to easily outperform VMware in both storage and CPU performance while costing… nothing unless you purchase support!</p><h2 id=direct-disk-access>Direct disk access</h2><p>It turns out, that you can add custom udev rules to pass through devices directly to VMs.</p><p>For example, assuming <code>/dev/sdb</code> is the disk you wish to make available to guests, you can edit <code>/etc/udev/rules.d/50-udev.rules</code> like so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>ACTION==&#34;add&#34;, KERNEL==&#34;sdb&#34;, SYMLINK+=&#34;xapi/block/%k&#34;, RUN+=&#34;/bin/sh -c &#39;/opt/xensource/libexec/local-device-change %k 2&gt;&amp;1 &gt;/dev/null&amp;&#39;&#34;
</span></span><span style=display:flex><span>ACTION==&#34;remove&#34;, KERNEL==&#34;sdb&#34;, RUN+=&#34;/bin/sh -c &#39;/opt/xensource/libexec/local-device-change %k 2&gt;&amp;1 &gt;/dev/null&amp;&#39;&#34;
</span></span></code></pre></div><p>There are some limitations with this:</p><p>You&rsquo;re passing through the whole disk and can only grant that entire disk to a single VM.
The udev configuration is volitile and is likely to be lost upon installing XenServer updates.
You cannot (to my knowledge) boot from directly attached storage devices.
What I&rsquo;ve actually done is partitioned the SSD RAID array into 4 paritions on the XenServer host, this allows me to carve up the SSD RAID array and present adiquit storage to several VMs on the host.
i.e.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>ACTION==&#34;add&#34;, KERNEL==&#34;sdb1&#34;, SYMLINK+=&#34;xapi/block/%k&#34;, RUN+=&#34;/bin/sh -c &#39;/opt/xensource/libexec/local-device-change %k 2&gt;&amp;1 &gt;/dev/null&amp;&#39;&#34;
</span></span><span style=display:flex><span>ACTION==&#34;remove&#34;, KERNEL==&#34;sdb1&#34;, RUN+=&#34;/bin/sh -c &#39;/opt/xensource/libexec/local-device-change %k 2&gt;&amp;1 &gt;/dev/null&amp;&#39;&#34;
</span></span><span style=display:flex><span>ACTION==&#34;add&#34;, KERNEL==&#34;sdb2&#34;, SYMLINK+=&#34;xapi/block/%k&#34;, RUN+=&#34;/bin/sh -c &#39;/opt/xensource/libexec/local-device-change %k 2&gt;&amp;1 &gt;/dev/null&amp;&#39;&#34;
</span></span><span style=display:flex><span>ACTION==&#34;remove&#34;, KERNEL==&#34;sdb2&#34;, RUN+=&#34;/bin/sh -c &#39;/opt/xensource/libexec/local-device-change %k 2&gt;&amp;1 &gt;/dev/null&amp;&#39;&#34;
</span></span></code></pre></div><p>etc…</p><p>I&rsquo;ve then:</p><ul><li>Mapped each partition to a VM running on that host.</li><li>Partitioned it within the host as needed.</li><li>Mounted /var/lib/docker as a BTRFS volume with compression enabled (compress=lzo) - This is used by CI as we build our our applications in Docker.</li><li>Mounted /home as an EXT4 - This is used by Gitlab-CI to checkout the pre-reqs for each build.</li></ul><h2 id=performance-benchmarks>Performance (benchmarks)</h2><h3 id=observations>Observations</h3><p>The HP P410i RAID card on the old G6 DL360&rsquo;s I am using for this is far underpowered and is unable to perform anywhere near the SSD&rsquo;s rated speeds.
Direct disk access is 2-6 times ‘faster&rsquo; than XenServer&rsquo;s standard LVM or EXT3 storage.
There is less than a 10% difference in performance between the SSD RAID array on the VM with the presented disk than directly on the XenServer host.
XenServer&rsquo;s raw disk performance was exactly the same as Debian 7.
Remember to ensure your RAID (and disk cache if not in prod) is enabled!
Points of Note:</p><p>This is an old server, we&rsquo;re not talking your latest and greatest hardware here, we&rsquo;re talking giving new life to an old-ish dog so that it may retain its usefulness while remaining cost effective.
With TRIM being unavailable when using RAID, it is expected that the write performance will decrease somewhat overtime as the disks fill up as the disks will have to perform a ‘READ, ERASE, WRITE&rsquo; rather than a simple WRITE, To aid the lack of TRIM, I have left more than 25% of the disks unused as we simply don&rsquo;t need 1TB of SSD storage on each of the hosts.
We have some 1GB cache cards arriving in the following weeks which we will upgrade to from the 512MB cards presently installed - I expect this to significantly further improve performance.</p><h2 id=hardware>Hardware</h2><ul><li>Refurbished HP DL360 G6 (2010 model).</li><li>2x 8 Core Xeon x5550 w/ hyperthreading, 8 cores presented to domU.</li><li>96GB DDR3 in dom0, 4GB in domU.</li><li>HP P410i w/ 512MB cache.</li><li>2x 10K 146GB spindles for dom0.</li><li>‘HP Genuine'!</li><li>Constant 5-6 Watts each + cooling.</li><li>2x SanDisk Extreme Pro SSD 480GB in RAID 0</li><li>SATA III, Read up to 550MB/s, Write up to 515MB/s, Random Read up to 100K IOPS, Random Write up to 90K IOPS.</li><li>0.15-0.2 Watts peak each.</li></ul><p>EXT4 Bonnie++ Results w/ 2x SSD in RAID1, XenServer 6.5 RC1, LVM:</p><p>![]({{ site.url }}/img/xenserver/dl360-lvm.jpg)</p><p>EXT4 Bonnie++ Results w/ 2x SSD in RAID1, XenServer 6.5 RC1, Direct Disk:</p><p>![]({{ site.url }}/img/xenserver/dl360-dd.jpg)</p><p>EXT4 dd Results w/ 2x SSD in RAID0, XenServer 6.5 RC1, LVM:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>samm@serv-dhcp-13:/var/tmp# dd if=/dev/zero of=tempfile bs=1M count=8000 conv=fdatasync,notrunc
</span></span><span style=display:flex><span> 8000+0 records in
</span></span><span style=display:flex><span> 8000+0 records out
</span></span><span style=display:flex><span> 8388608000 bytes (8.4 GB) copied, 118.11 s, 71.0 MB/s
</span></span></code></pre></div><p>EXT4 dd Results w/ 2x SSD in RAID0, XenServer 6.5 RC1, Direct Disk:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>samm@serv-dhcp-13:/var/tmp# dd if=/dev/zero of=tempfile bs=1M count=8000 conv=fdatasync,notrunc
</span></span><span style=display:flex><span> 8000+0 records in
</span></span><span style=display:flex><span> 8000+0 records out
</span></span><span style=display:flex><span> 8388608000 bytes (8.4 GB) copied, 19.21 s, 437 MB/s
</span></span></code></pre></div><p>Future Benchmarking Steps</p><ul><li>Observe performance over time since TRIM is not available.</li><li>Upgrade RAID cards to 1GB cache (we have some spares).</li><li>Try with our new Gen8 BL460c blade servers, with locally attached P420i RAID controllers and 2GB cache.</li><li>Try linux software RAID with direct disks.</li></ul><h2 id=xenservers-future>XenServer&rsquo;s Future</h2><p>Filesystems</p><p>I would hope that both EXT4 (and BTRFS) support is added in the near future.
With this I would expect auto-detecting TRIM support similar to VMware and Hyper-V.
Direct Disk Access</p><p>Direct disk access clearly offers massive performance gains, I would like to see XenServer add this as an easy to use option when configuring storage.
Non-volitile advanced configuration</p><p>Related to direct disk access, XenServer needs some form of non-volitle advanced configuration options.
VMware, raw Xen and KVM let you tweak many options without risk of loss after installing minor updates.</p></div></article><hr><p class=articleTagsContainer><span></span>
<strong>Tags:</strong>
<a class=buttonTag href=../../../tags/hardware>#hardware</a>
<a class=buttonTag href=../../../tags/storage>#storage</a>
<a class=buttonTag href=../../../tags/tech>#tech</a></p><div class=relatedArticlesContainer><hr><h2>More posts like this</h2><div class=postlist><article class="card postlistitem"><div><h2><a href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/>Making Work Visible - Avoid DMs</a></h2><p class=date><span title=Date></span>
2022-10-20
|
<span title=Tags></span>
<a href=../../../tags/communication>#communication</a>
<a href=../../../tags/culture>#culture</a>
<a href=../../../tags/tech>#tech</a>
<a href=../../../tags/work>#work</a></p><a class=unstyledLink href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/><img src=https://smcleod.net/2022/10/making-work-visible-avoid-dms//dm-disruption.jpg alt></a><div class=articlePreview><p><h3 id=avoid-sending-dmspms-for-support-and-technical-questions>Avoid sending DMs/PMs for support and technical questions</h3><p>This is a blurb I usually add to the wiki pages of teams I work with, it&rsquo;s a simple reminder that we should avoid sending direct messages (or worse - calling) individuals for support and technical questions.</p><p>Instead, we should use the appropriate channels for the team. This is a simple way to make work visible, share knowledge, reduce context switching, avoid silos and share the support load.</p></p><p><a href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/>Continue reading </a></p></div></div><hr></article><article class="card postlistitem"><div><h2><a href=https://smcleod.net/2021/07/rancilio-silvia-upgrade/>Rancilio Silvia Upgrade</a></h2><p class=date><span title=Date></span>
2021-07-11
|
<span title=Tags></span>
<a href=../../../tags/coffee>#coffee</a>
<a href=../../../tags/hardware>#hardware</a></p><a class=unstyledLink href=https://smcleod.net/2021/07/rancilio-silvia-upgrade/><img src=https://smcleod.net/2021/07/rancilio-silvia-upgrade//assembled_preview.jpeg alt></a><div class=articlePreview><p><p>Weekend upgrades to my Ranchilio Silvia v4 espresso machine.</p><ul><li>Added a digital <a href=https://www.seattlecoffeegear.com/blog/2018/10/01/whats-a-pid/>PID</a> <a href=https://www.jetblackespresso.com.au/shop/p/rancilio-silvia-pid-preinfusion/>kit</a> for improved temperature control</li><li>Replaced the boiler element</li><li>Thermal insulation</li><li>Basic <a href=https://www.cygnett.com/products/smart-wi-fi-plug-with-power-monitoring>&ldquo;smart&rdquo; power control</a> with Siri integration</li><li>Upgrade the standard Ranchilio basket to a <a href=https://www.dukescoffee.com.au/shop/vst-precision-basket-ridged/>VST 18g Ridged</a></li></ul></p><p><a href=https://smcleod.net/2021/07/rancilio-silvia-upgrade/>Continue reading </a></p></div></div><hr></article></div></div></main><footer><hr><div class=footerColumns><ul class=notlist><li><strong>Links</strong></li><li><a target=_blank href=https://github.com/sammcj>Github | @sammcj</a></li><li><a target=_blank href=https://twitter.com/s_mcleod>Twitter | @s_mcleod</a></li><li><a target=_blank href=https://smcleod.net>This Website | smcleod.net</a></li></ul></div><p><small>2022 &copy; Sam McLeod</small></p><p><small></small></p></footer></div></div></div></body></html>