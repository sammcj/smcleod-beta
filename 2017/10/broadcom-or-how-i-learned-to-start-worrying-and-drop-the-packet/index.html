<!doctype html><html lang=en-au><head><meta name=viewport content="width=device-width"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=7"><link rel=icon href=../../../favicon.png><link rel="shortcut icon" href=../../../favicon.ico type=image/x-icon><link rel=apple-touch-icon href=../../../apple-touch-icon.png><link rel=icon href=../../../logo.svg type=image/svg+xml><title>Broadcom, Or How I Learned To Start Worrying And Drop The Packet &ndash;
smcleod.net</title><link href=../../../symbols-nerd-font/symbols-nerd-font.css rel=stylesheet><link href=../../../jetbrains-mono/jetbrains-mono.css rel=stylesheet><link type=text/css rel=stylesheet href=https://smcleod.net/css/styles.66349763506489d57ba916836ca1c45e0ac3f667fe4f2a152cf7619a81ad51a060782c6231e1a885e311c2fd9fdc72e95b340fbd9530152e367d478fe56978c5.css integrity="sha512-ZjSXY1BkidV7qRaDbKHEXgrD9mf+TyoVLPdhmoGtUaBgeCxiMeGoheMRwv2f3HLpWzQPvZUwFS42fUeP5Wl4xQ=="><link href=../../../atkinson-hyperlegible/atkinson-hyperlegible.css rel=stylesheet><meta name=author content="Sam McLeod"><meta name=keywords content="hardware,networking,tech"><meta name=description content=" we started the process to upgrade one of our hypervisor compute clusters when we encountered a rather painful bug with HP&amp;rsquo;s Broadcom NIC chipsets.</p>
<p>We were part way through a routine rolling pool upgrade of our hypervisor (XenServer) cluster when we observed unexpected and intermittent loss of connectivity between several VMs, then entire XenServer hosts.</p>
<p>The problems appeared to impact hosts that hadn&amp;rsquo;t yet upgraded to XenServer 7.2. We now attribute this to a symptom of extreme packet loss between the hosts in the pool and thanks to buggy firmware from Broadcom and HP.</p>"><meta property="og:site_name" content="smcleod.net"><meta property="og:title" content="Broadcom, Or How I Learned To Start Worrying And Drop The Packet"><meta property="og:type" content="article"><meta property="article:author" content="Sam McLeod"><meta property="article:published_time" content="2017-10-13T00:00:00Z+0000"><meta property="article:tag" content="hardware"><meta property="article:tag" content="networking"><meta property="article:tag" content="tech"><meta property="og:url" content="https://smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/"><meta property="og:image" content="https://smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet//office-space-broadcom.jpg"><meta property="og:description" content="<p>Earlier this week we started the process to upgrade one of our hypervisor compute clusters when we encountered a rather painful bug with HP&amp;rsquo;s Broadcom "><meta name=twitter:card content="summary_large_image"><meta property="twitter:domain" content="mcleod.ne"><meta property="twitter:url" content="https://smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/"><meta name=twitter:title content="Broadcom, Or How I Learned To Start Worrying And Drop The Packet"><meta name=twitter:image content="https://smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet//office-space-broadcom.jpg"><meta name=twitter:description content="<p>Earlier this week we started the process to upgrade one of our hypervisor compute clusters when we encountered a rather painful bug with HP&amp;rsquo;s Broadcom "><link rel=manifest href=../../../manifest/index.json></head><body><div id=baseContainer><header><div class=titleAndSearchContainer><div id=titleContainer><a class=unstyledLink href=../../../><img src=../../../logo.svg alt=Logo></a><div class=rightOfLogo><div class=titleAndHamburger><h1><a class=unstyledLink href=../../../>smcleod.net</a></h1></div><div id=wide_nav><nav><ul id=main-nav><li><a href=../../../>Home</a></li><li><a href=../../../posts>Posts</a></li><li><a href=https://smcleod.net/pages/about/>About</a></li><li><a href=https://smcleod.net/pages/links/>Links</a></li><li><a href=../../../tags>Tags</a></li><li><a href=../../../search>Search</a></li></ul></nav></div></div></div><div class=search><input id=searchbar type=text placeholder=Search>
<a class=nerdlink onclick=newSearch()>&#xf002;</a></div><script>function newSearch(){let e=searchbar.value.trim();if(!e)return;location.href=`/search?q=${e}`}searchbar.onkeyup=e=>{e.keyCode==13&&newSearch()}</script></div><div id=links><a rel=noreferrer target=_blank class=nerdlink href=../../../index.xml>&#xf09e;
<span>RSS</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://github.com/sammcj>&#xf09b;
<span>Github</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://twitter.com/s_mcleod>&#xf099;
<span>Twitter</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://linkedin.com/in/sammcj>&#xf0e1;
<span>LinkedIn</span></a></div></header><div id=contentContainer><div id=content><main><article class="card single"><h1>Broadcom, Or How I Learned To Start Worrying And Drop The Packet</h1><p class=date><span title=Date></span>
2017-10-13</p><figure style=margin:0><img src=https://smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet//office-space-broadcom.jpg alt></figure><div><p>Earlier this week we started the process to upgrade one of our hypervisor compute clusters when we encountered a rather painful bug with HP&rsquo;s Broadcom NIC chipsets.</p><p>We were part way through a routine rolling pool upgrade of our hypervisor (XenServer) cluster when we observed unexpected and intermittent loss of connectivity between several VMs, then entire XenServer hosts.</p><p>The problems appeared to impact hosts that hadn&rsquo;t yet upgraded to XenServer 7.2. We now attribute this to a symptom of extreme packet loss between the hosts in the pool and thanks to buggy firmware from Broadcom and HP.</p><p>We were aware of the <a href=http://www.thevirtualist.org/bricked-qlogic-broadcom-bcm57840-driver-update/>recently published issues</a> with Broadcom/HP NICs used in VMware clusters where NICs would be <a href="https://h20566.www2.hpe.com/hpsc/doc/public/display?docId=a00027033en_us">bricked by a firmware upgrade</a>. This issue is different from what we experienced.</p><p>We experienced extreme packet loss between hosts in the cluster. With XenServer, the pool master must be upgraded first. The result was that XAPI pool management suffered a communication breakdown across the management network and complicated diagnosis. In fact, the connectivity problems went unnoticed until many hours after the master was upgraded.</p><p>At first appeared as if it was a problem caused by the pool being partially upgraded.</p><p>We wondered if we had perhaps made a poor decision to run the upgrade on a single node for a few hours to observe its performance. We made the call to upgrade another host and analyse our findings.</p><p>The next upgraded hosts appeared stable. In fact we later found this host wasn&rsquo;t impacted by the bug. We then made the call to upgrade several more nodes and continue to track their stability.</p><p>After upgrading half the pool, we suddenly hit problems. VMs failed, Hosts started dropping out of the pool and losing track of the power state of running/stopped VMs.</p><p>We found that the master along with one of the other hosts were experiencing major packet loss on their management network cards. We suspected faulty NICs as it wouldn&rsquo;t be the first time a Broadcom had failed us and there is no physical network cabling.</p><p>Broadcom has had its fair share of bad press over the years. Many botched firmware updates and proprietary driver issues. I&rsquo;m recommending people to stay clear from using network cards based on their chipsets.</p><h2 id=downgrading-the-firmware>Downgrading The Firmware</h2><p>As soon as we spotted the packet loss on the Broadcom NICs we upgraded their firmware to <a href=http://downloads.linux.hpe.com/SDR/repo/spp/RHEL/7/x86_64/current/firmware-nic-qlogic-nx2-2.19.22-1.1.x86_64.rpm>2.19.22-1</a> with no improvement.
We then upgraded to <a href=http://downloads.linux.hpe.com/SDR/repo/spp/RHEL/7/x86_64/current/hp-firmware-nic-qlogic-nx2-2.18.44-1.1.x86_64.rpm>2.18.44-1 / 7.14.62</a> again with no improvement.
We even went as far as trying <a href=http://downloads.linux.hpe.com/SDR/repo/spp/RHEL/7/x86_64/current/hp-firmware-nic-qlogic-nx2-2.16.20-1.1.x86_64.rpm>2.16.20 / 7.12.83</a> from back in 2015 - but still no luck.</p><p>At the time of writing this no firmware downgrades (or upgrades) have fixed the issue.</p><p>The packet loss manifests itself immediately after rebooting or power cycling. But - <em>not on every reboot!</em>. This is the odd thing - approximately half the time when booting a host it is fine until the next boot.</p><p>We&rsquo;ve compared the <code>dmesg</code>, <code>lspci</code> and <code>modinfo</code> output between boot cycles, we can&rsquo;t find anything that stands out.</p><p>The bug seems to be caused by the version of the <code>bnx2x</code> driver present in XenServer 7.2&rsquo;s Kernel. Upon further reading HP recommends that you use bnx2x driver 7.14.29-2 or later, XenServer still uses the old Kernel version of 4.4.0 - that&rsquo;s not currently an option.</p><p>I suspect that it&rsquo;s a bug in the Broadcom firmware loaded into the NIC upon boot.
I suspect a race condition related to the devices interrupt handling (MSI/MSI-X).</p><h2 id=xenserver>XenServer</h2><p>XenServer needs to update its kernel or at least the bnx2x driver module past the version that triggers the bug. I&rsquo;ve logged a ticket for this over at <a href=https://bugs.xenserver.org/browse/XSO-808>bugs.xenserver.org</a></p><p>Additionally, XenServer didn&rsquo;t notice the packet loss/network interruptions during the rolling pool upgrade. I have reported this concern and have suggested that XenServer adds pool wide checks for connectivity issues between hosts, at <em>least</em> during a pool upgrade.</p><h2 id=workaround>Workaround</h2><p>We don&rsquo;t have (a good) one.</p><p>Currently we&rsquo;re simply testing for packet loss after boot on the management NIC. If detected we reboot the host and check again. This far from ideal - but until the bug is resolved there isn&rsquo;t any other fix that we can find short of compiling a custom module for XenServer 7.2.</p><p>Given the widespread problems with Broadcom, we&rsquo;ve ordered HP 560M, Intel based NICs to replace them.</p><h2 id=bnx2x-driver>BNX2X Driver</h2><p>The driver included with XenServer 7.2 that triggers the problem is <code>1.714.1</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>filename:       /lib/modules/4.4.0+10/updates/bnx2x.ko
</span></span><span style=display:flex><span>version:        1.714.1
</span></span><span style=display:flex><span>license:        GPL
</span></span><span style=display:flex><span>#description:   QLogic BCM57710/57711/57711E/57712/57712_MF/57800/57800_MF/57810/57810_MF/57840/57840_MF Driver
</span></span><span style=display:flex><span>author:         Eliezer Tamir
</span></span><span style=display:flex><span>srcversion:     927337210F53311B18D0D7E
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000163Fsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000163Esv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000163Dsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v00001077d000016ADsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016ADsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v00001077d000016A4sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A4sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016ABsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016AFsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A2sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v00001077d000016A1sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A1sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000168Dsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016AEsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000168Esv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A9sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A5sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000168Asv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000166Fsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d00001663sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d00001662sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d00001650sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000164Fsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000164Esv*sd*bc*sc*i*
</span></span><span style=display:flex><span>depends:        mdio,libcrc32c,ptp,vxlan
</span></span><span style=display:flex><span>vermagic:       4.4.0+10 SMP mod_unload modversions
</span></span><span style=display:flex><span>parm:           pri_map: Priority to HW queue mapping (uint)
</span></span><span style=display:flex><span>parm:           num_queues: Set number of queues (default is as a number of CPUs) (int)
</span></span><span style=display:flex><span>parm:           disable_iscsi_ooo: Disable iSCSI OOO support (uint)
</span></span><span style=display:flex><span>parm:           disable_tpa: Disable the TPA (LRO) feature (uint)
</span></span><span style=display:flex><span>parm:           int_mode: Force interrupt mode other than MSI-X (1 INT#x; 2 MSI) (uint)
</span></span><span style=display:flex><span>parm:           dropless_fc: Pause on exhausted host ring (uint)
</span></span><span style=display:flex><span>parm:           poll: Use polling (for debug) (uint)
</span></span><span style=display:flex><span>parm:           mrrs: Force Max Read Req Size (0..3) (for debug) (int)
</span></span><span style=display:flex><span>parm:           debug: Default debug msglevel (uint)
</span></span><span style=display:flex><span>parm:           num_vfs: Number of supported virtual functions (0 means SR-IOV is disabled) (uint)
</span></span><span style=display:flex><span>parm:           autogreeen: Set autoGrEEEn (0:HW default; 1:force on; 2:force off) (uint)
</span></span><span style=display:flex><span>parm:           native_eee:int
</span></span><span style=display:flex><span>parm:           eee:set EEE Tx LPI timer with this value; 0: HW default; -1: Force disable EEE.
</span></span><span style=display:flex><span>parm:           tx_switching: Enable tx-switching (uint)
</span></span></code></pre></div><p>Whereas XenServer 7.0 has driver version <code>1.713.04</code> which seems not to trigger the issue:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>filename:       /lib/modules/3.10.0+10/extra/bnx2x.ko
</span></span><span style=display:flex><span>version:        1.713.04
</span></span><span style=display:flex><span>license:        GPL
</span></span><span style=display:flex><span>#description:   QLogic BCM57710/57711/57711E/57712/57712_MF/57800/57800_MF/57810/57810_MF/57840/57840_MF Driver
</span></span><span style=display:flex><span>author:         Eliezer Tamir
</span></span><span style=display:flex><span>srcversion:     13EAA521200A40118055D63
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000163Fsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000163Esv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000163Dsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v00001077d000016ADsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016ADsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v00001077d000016A4sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A4sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016ABsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016AFsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A2sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v00001077d000016A1sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A1sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000168Dsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016AEsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000168Esv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A9sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d000016A5sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000168Asv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000166Fsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d00001663sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d00001662sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d00001650sv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000164Fsv*sd*bc*sc*i*
</span></span><span style=display:flex><span>alias:          pci:v000014E4d0000164Esv*sd*bc*sc*i*
</span></span><span style=display:flex><span>depends:        mdio,libcrc32c,ptp
</span></span><span style=display:flex><span>vermagic:       3.10.0+10 SMP mod_unload modversions
</span></span><span style=display:flex><span>parm:           pri_map: Priority to HW queue mapping (uint)
</span></span><span style=display:flex><span>parm:           num_queues: Set number of queues (default is as a number of CPUs) (int)
</span></span><span style=display:flex><span>parm:           disable_iscsi_ooo: Disable iSCSI OOO support (uint)
</span></span><span style=display:flex><span>parm:           disable_tpa: Disable the TPA (LRO) feature (uint)
</span></span><span style=display:flex><span>parm:           int_mode: Force interrupt mode other than MSI-X (1 INT#x; 2 MSI) (uint)
</span></span><span style=display:flex><span>parm:           dropless_fc: Pause on exhausted host ring (uint)
</span></span><span style=display:flex><span>parm:           poll: Use polling (for debug) (uint)
</span></span><span style=display:flex><span>parm:           mrrs: Force Max Read Req Size (0..3) (for debug) (int)
</span></span><span style=display:flex><span>parm:           debug: Default debug msglevel (uint)
</span></span><span style=display:flex><span>parm:           num_vfs: Number of supported virtual functions (0 means SR-IOV is disabled) (uint)
</span></span><span style=display:flex><span>parm:           autogreeen: Set autoGrEEEn (0:HW default; 1:force on; 2:force off) (uint)
</span></span><span style=display:flex><span>parm:           native_eee:int
</span></span><span style=display:flex><span>parm:           eee:set EEE Tx LPI timer with this value; 0: HW default; -1: Force disable EEE.
</span></span><span style=display:flex><span>parm:           tx_switching: Enable tx-switching (uint)
</span></span></code></pre></div><h2 id=affected-components>Affected Components</h2><ul><li><a href="https://www.hpe.com/h20195/v2/getpdf.aspx/c04111538.pdf?ver=3">HP 530M Network cards</a> (as they use the Broadcom bcm57810 chipset), commonly found in BL460c Gen8 blades and similar.</li><li>XenServer 7.2 (Patched to the latest XS72E006 patch)</li><li>Kernel 4.4.0+10 as found in XenServer 7.2</li><li>Broadcom bnx2x module version 1.714.1</li><li><a href="http://h20564.www2.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3bc2b88453424d87b7543d6459">HP firmware for qlogic nx2</a> (seemingly all versions)</li></ul><h2 id=links>Links</h2><ul><li><a href=http://blog.serverfault.com/post/broadcom-die-mutha/>Broadcom, Die Mutha</a></li><li><a href=http://www.thevirtualist.org/bricked-qlogic-broadcom-bcm57840-driver-update/>Bricked QLogic Broadcom BCM57840 after driver update</a></li><li><a href="https://www.hpe.com/h20195/v2/getpdf.aspx/c04111538.pdf?ver=3">HP Flex-10 10Gb 2-port 530M Adapter</a></li><li><a href="https://h20566.www2.hpe.com/hpsc/doc/public/display?docId=a00027033en_us">HPE Network Adapters - Updating The BNX2X Driver Package Version 2.713.30 On VMware Hosts With Certain Network Adapters Running Certain Firmware May Require A Network Adapter Replacement</a></li><li><a href="http://h20564.www2.hpe.com/hpsc/swd/public/detail?swItemId=MTX_19bc1fb428d4400a90d59a7175#tab-history">HP Qlogic NX2 Firmware</a></li></ul></div></article><hr><p class=articleTagsContainer><span></span>
<strong>Tags:</strong>
<a class=buttonTag href=../../../tags/hardware>#hardware</a>
<a class=buttonTag href=../../../tags/networking>#networking</a>
<a class=buttonTag href=../../../tags/tech>#tech</a></p><div class=relatedArticlesContainer><hr><h2>More posts like this</h2><div class=postlist><article class="card postlistitem"><div><h2><a href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/>Making Work Visible - Avoid DMs</a></h2><p class=date><span title=Date></span>
2022-10-20
|
<span title=Tags></span>
<a href=../../../tags/communication>#communication</a>
<a href=../../../tags/culture>#culture</a>
<a href=../../../tags/tech>#tech</a>
<a href=../../../tags/work>#work</a></p><a class=unstyledLink href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/><img src=https://smcleod.net/2022/10/making-work-visible-avoid-dms//dm-disruption.jpg alt></a><div class=articlePreview><p><h3 id=avoid-sending-dmspms-for-support-and-technical-questions>Avoid sending DMs/PMs for support and technical questions</h3><p>This is a blurb I usually add to the wiki pages of teams I work with, it&rsquo;s a simple reminder that we should avoid sending direct messages (or worse - calling) individuals for support and technical questions.</p><p>Instead, we should use the appropriate channels for the team. This is a simple way to make work visible, share knowledge, reduce context switching, avoid silos and share the support load.</p></p><p><a href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/>Continue reading </a></p></div></div><hr></article><article class="card postlistitem"><div><h2><a href=https://smcleod.net/2021/07/rancilio-silvia-upgrade/>Rancilio Silvia Upgrade</a></h2><p class=date><span title=Date></span>
2021-07-11
|
<span title=Tags></span>
<a href=../../../tags/coffee>#coffee</a>
<a href=../../../tags/hardware>#hardware</a></p><a class=unstyledLink href=https://smcleod.net/2021/07/rancilio-silvia-upgrade/><img src=https://smcleod.net/2021/07/rancilio-silvia-upgrade//assembled_preview.jpeg alt></a><div class=articlePreview><p><p>Weekend upgrades to my Ranchilio Silvia v4 espresso machine.</p><ul><li>Added a digital <a href=https://www.seattlecoffeegear.com/blog/2018/10/01/whats-a-pid/>PID</a> <a href=https://www.jetblackespresso.com.au/shop/p/rancilio-silvia-pid-preinfusion/>kit</a> for improved temperature control</li><li>Replaced the boiler element</li><li>Thermal insulation</li><li>Basic <a href=https://www.cygnett.com/products/smart-wi-fi-plug-with-power-monitoring>&ldquo;smart&rdquo; power control</a> with Siri integration</li><li>Upgrade the standard Ranchilio basket to a <a href=https://www.dukescoffee.com.au/shop/vst-precision-basket-ridged/>VST 18g Ridged</a></li></ul></p><p><a href=https://smcleod.net/2021/07/rancilio-silvia-upgrade/>Continue reading </a></p></div></div><hr></article></div></div></main><footer><hr><div class=footerColumns><ul class=notlist><li><strong>Links</strong></li><li><a target=_blank href=https://github.com/sammcj>Github | @sammcj</a></li><li><a target=_blank href=https://twitter.com/s_mcleod>Twitter | @s_mcleod</a></li><li><a target=_blank href=https://smcleod.net>This Website | smcleod.net</a></li></ul></div><p><small>2022 &copy; Sam McLeod</small></p><p><small></small></p></footer></div></div></div></body></html>