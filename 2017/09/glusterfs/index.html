<!doctype html><html lang=en-au><head><meta name=viewport content="width=device-width"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=7"><link rel=icon href=../../../favicon.png><link rel="shortcut icon" href=../../../favicon.ico type=image/x-icon><link rel=apple-touch-icon href=../../../apple-touch-icon.png><link rel=icon href=../../../logo.svg type=image/svg+xml><title>GlusterFS &ndash;
smcleod.net</title><link href=../../../symbols-nerd-font/symbols-nerd-font.css rel=stylesheet><link href=../../../jetbrains-mono/jetbrains-mono.css rel=stylesheet><link type=text/css rel=stylesheet href=https://smcleod.net/css/styles.b7c7d7b24065587a8a3ead6386742f73f58902b87b58335013f2e71850ffcf2b1c6e122afd9779105debf414b583702e78b2598f2f707160b6d928da99614cbc.css integrity="sha512-t8fXskBlWHqKPq1jhnQvc/WJArh7WDNQE/LnGFD/zyscbhIq/Zd5EF3r9BS1g3AueLJZjy9wcWC22SjamWFMvA=="><meta name=author content="Sam McLeod"><meta name=keywords content="software,storage,tech"><meta name=description content="e process of shifting from using our custom &amp;lsquo;glue&amp;rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&amp;rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.</p>
<p>The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.</p>
<p>I spent time evaluating various cluster filesystems / network block storage and the two that stood out were Ceph and Gluster and settled on Gluster as the most suitable for our needs, it&amp;rsquo;s far less complex to deploy than Ceph, it has less moving pieces and files are stored in a familiar manner on hosts.</p>"><meta property="og:site_name" content="smcleod.net"><meta property="og:title" content="GlusterFS"><meta property="og:type" content="article"><meta property="article:author" content="Sam McLeod"><meta property="article:published_time" content="2017-09-25T00:00:00Z+0000"><meta property="article:tag" content="software"><meta property="article:tag" content="storage"><meta property="article:tag" content="tech"><meta property="og:url" content="https://smcleod.net/2017/09/glusterfs/"><meta property="og:image" content="https://smcleod.net/2017/09/glusterfs//backdrop-insidebuilding.jpg"><meta property="og:description" content="<p>We&amp;rsquo;re in the process of shifting from using our custom &amp;lsquo;glue&amp;rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Do"><meta name=twitter:card content="summary_large_image"><meta property="twitter:domain" content="mcleod.ne"><meta property="twitter:url" content="https://smcleod.net/2017/09/glusterfs/"><meta name=twitter:title content="GlusterFS"><meta name=twitter:image content="https://smcleod.net/2017/09/glusterfs//backdrop-insidebuilding.jpg"><meta name=twitter:description content="<p>We&amp;rsquo;re in the process of shifting from using our custom &amp;lsquo;glue&amp;rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Do"><link rel=manifest href=../../../manifest/index.json></head><body><div id=baseContainer><header><div class=titleAndSearchContainer><div id=titleContainer><a class=unstyledLink href=../../../><img src=../../../logo.svg alt=Logo></a><div class=rightOfLogo><div class=titleAndHamburger><h1><a class=unstyledLink href=../../../>smcleod.net</a></h1></div><div id=wide_nav><nav><ul id=main-nav><li><a href=../../../>Home</a></li><li><a href=../../../posts>Posts</a></li><li><a href=https://smcleod.net/pages/about/>About</a></li><li><a href=https://smcleod.net/pages/links/>Links</a></li><li><a href=../../../tags>Tags</a></li><li><a href=../../../search>Search</a></li></ul></nav></div></div></div><div class=search><input id=searchbar type=text placeholder=Search>
<a class=nerdlink onclick=newSearch()>&#xf002;</a></div><script>function newSearch(){let e=searchbar.value.trim();if(!e)return;location.href=`/search?q=${e}`}searchbar.onkeyup=e=>{e.keyCode==13&&newSearch()}</script></div><div id=links><a rel=noreferrer target=_blank class=nerdlink href=../../../index.xml>&#xf09e;
<span>RSS</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://github.com/sammcj>&#xf09b;
<span>Github</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://twitter.com/s_mcleod>&#xf099;
<span>Twitter</span></a>
<a rel=noreferrer target=_blank class=nerdlink href=https://linkedin.com/in/sammcj>&#xf0e1;
<span>LinkedIn</span></a></div></header><div id=contentContainer><div id=content><main><article class="card single"><h1>GlusterFS</h1><p class=date><span title=Date>ï—¬</span>
2017-09-25</p><figure style=margin:0><img src=https://smcleod.net/2017/09/glusterfs//backdrop-insidebuilding.jpg alt></figure><div><p>We&rsquo;re in the process of shifting from using our custom &lsquo;glue&rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.</p><p>The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.</p><p>I spent time evaluating various cluster filesystems / network block storage and the two that stood out were Ceph and Gluster and settled on Gluster as the most suitable for our needs, it&rsquo;s far less complex to deploy than Ceph, it has less moving pieces and files are stored in a familiar manner on hosts.</p><h2 id=implementation>Implementation</h2><p>I&rsquo;ve settled on a 3 node deployment with one node as an <a href=http://docs.gluster.org/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/>arbiter</a> (replica 3, arbiter 1).</p><p>Our nodes are CentOS 7 VMs within our exiting XenServer infrastructure, each node has 8 vCPUs <a href=https://ark.intel.com/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz>Xeon E5-2680 v4</a>, 16GB of RAM and backed by our <a href=https://smcleod.net/tech/ssd-storage-cluster-diagram/>iSCSI SSD Storage</a>.</p><h2 id=automation--puppet>Automation / Puppet</h2><p>We&rsquo;re long time, heavy users of Puppet so naturally I&rsquo;m deploying Gluster via a <a href=https://github.com/voxpupuli/puppet-gluster>Puppet module</a> and generating volumes and volume configuration from <a href=https://docs.puppet.com/hiera/>Hiera</a>.</p><p>Volumes are automatically generated from the Hiera structure that defines our applications.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># profiles::services::gluster::volume:
</span></span><span style=display:flex><span>profiles::services::gluster::volume::pool: &#34;%{alias(&#39;profiles::services::gluster::host::pool&#39;)}&#34;
</span></span><span style=display:flex><span>profiles::services::gluster::volume::pool_members: &#34;%{alias(&#39;profiles::services::gluster::host::pool_members&#39;)}&#34;
</span></span><span style=display:flex><span>profiles::services::gluster::volume::brick_mountpoint: &#39;/mnt/gluster-storage&#39;
</span></span><span style=display:flex><span>profiles::services::gluster::volume::replica: 3
</span></span><span style=display:flex><span>profiles::services::gluster::volume::arbiter: 1
</span></span><span style=display:flex><span>profiles::services::gluster::volume::volume_options:
</span></span><span style=display:flex><span>  # Failover clients after 10 seconds of a server being unavailable
</span></span><span style=display:flex><span>  &#39;network.ping-timeout&#39;: &#39;10&#39;
</span></span><span style=display:flex><span>  &#39;cluster.lookup-optimize&#39;: &#39;true&#39;
</span></span><span style=display:flex><span>  &#39;cluster.readdir-optimize&#39;: &#39;true&#39;
</span></span><span style=display:flex><span>  &#39;cluster.use-compound-fops&#39;: &#39;true&#39;
</span></span><span style=display:flex><span>  &#39;performance.parallel-readdir&#39;: &#39;true&#39;
</span></span><span style=display:flex><span>  &#39;performance.client-io-threads&#39;: &#39;true&#39;
</span></span><span style=display:flex><span>  &#39;performance.stat-prefetch&#39;: &#39;true&#39;
</span></span><span style=display:flex><span>  &#39;diagnostics.brick-log-level&#39;: &#39;WARNING&#39;
</span></span><span style=display:flex><span>  &#39;diagnostics.client-log-level&#39;: &#39;WARNING&#39;
</span></span><span style=display:flex><span>  &#39;server.event-threads&#39;: &#39;3&#39;
</span></span><span style=display:flex><span>  &#39;client.event-threads&#39;: &#39;3&#39;
</span></span></code></pre></div><p>I enabled the <code>nis_enabled</code> SEbool to prevent a number of SELinux denials I noticed in the logs:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>profiles::os::redhat::centos7::selinux::selinux_booleans:
</span></span><span style=display:flex><span>  &#39;nis_enabled&#39;:
</span></span><span style=display:flex><span>    value: &#39;on&#39;
</span></span></code></pre></div><p>I also increased the local emepheral port range and the kernel&rsquo;s socket backlog limit as suggested by Redhat:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>sysctl::base::values:
</span></span><span style=display:flex><span>  &#39;net.ipv4.tcp_max_syn_backlog&#39;:
</span></span><span style=display:flex><span>    ensure: present
</span></span><span style=display:flex><span>    value: &#39;4096&#39;
</span></span><span style=display:flex><span>    comment: &#39;Increase syn backlogs for gluster&#39;
</span></span><span style=display:flex><span>  &#39;net.ipv4.ip_local_port_range&#39;:
</span></span><span style=display:flex><span>    ensure: present
</span></span><span style=display:flex><span>    value: &#39;32768 65535&#39;
</span></span><span style=display:flex><span>    comment: &#39;Increase local port range for gluster&#39;
</span></span><span style=display:flex><span>  &#39;net.core.somaxconn&#39;:
</span></span><span style=display:flex><span>    ensure: present
</span></span><span style=display:flex><span>    value: &#39;2048&#39;
</span></span><span style=display:flex><span>    comment: &#39;Increase kernel socket backlog limit for gluster&#39;
</span></span></code></pre></div><h2 id=performance>Performance</h2><p>Performance is, well, very poor for anything other than large reads.</p><p>I was expecting a hit to IOP/s performance as you would with any clustered, network file system, but I wasn&rsquo;t expecting it to drop as much as it did, especially after enabling the above performance options on the volumes.</p><p>![gluster perf vs native.jpg]({{site.baseurl}}/img/gluster perf vs native.jpg)</p><h3 id=sequential-read>Sequential Read</h3><p>Acceptable performance at 380MB/s:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=1M --iodepth=32 --size=5G --readwrite=read
</span></span><span style=display:flex><span>test: (g=0): rw=read, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=32
</span></span><span style=display:flex><span>fio-2.1.11
</span></span><span style=display:flex><span>Starting 1 process
</span></span><span style=display:flex><span>Jobs: 1 (f=1): [R(1)] [100.0% done] [380.0MB/0KB/0KB /s] [380/0/0 iops] [eta 00m:00s]
</span></span><span style=display:flex><span>test: (groupid=0, jobs=1): err= 0: pid=509: Mon Sep 25 12:17:06 2017
</span></span><span style=display:flex><span>  read : io=5120.0MB, bw=310321KB/s, iops=303, runt= 16895msec
</span></span><span style=display:flex><span>  cpu          : usr=0.58%, sys=3.50%, ctx=16068, majf=0, minf=533
</span></span><span style=display:flex><span>  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.2%, 16=0.3%, 32=99.4%, &gt;=64=0.0%
</span></span><span style=display:flex><span>     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
</span></span><span style=display:flex><span>     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, &gt;=64=0.0%
</span></span><span style=display:flex><span>     issued    : total=r=5120/w=0/d=0, short=r=0/w=0/d=0
</span></span><span style=display:flex><span>     latency   : target=0, window=0, percentile=100.00%, depth=32
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run status group 0 (all jobs):
</span></span><span style=display:flex><span>   READ: io=5120.0MB, aggrb=310321KB/s, minb=310321KB/s, maxb=310321KB/s, mint=16895msec, maxt=16895msec
</span></span></code></pre></div><h3 id=sequential-write>Sequential Write</h3><p>A terrible average of 25.6MB/s:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=1M --iodepth=32 --size=5G --readwrite=write
</span></span><span style=display:flex><span>test: (g=0): rw=write, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=32
</span></span><span style=display:flex><span>fio-2.1.11
</span></span><span style=display:flex><span>Starting 1 process
</span></span><span style=display:flex><span>test: Laying out IO file(s) (1 file(s) / 5120MB)
</span></span><span style=display:flex><span>Jobs: 1 (f=1): [W(1)] [18.9% done] [0KB/25600KB/0KB /s] [0/25/0 iops] [eta 01m:43s]
</span></span></code></pre></div><h3 id=4k-write-iops>4K Write IOP/s</h3><p>A terrible average of 684 IOP/s:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=32 --size=5G --readwrite=randwrite
</span></span><span style=display:flex><span>test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32
</span></span><span style=display:flex><span>fio-2.1.11
</span></span><span style=display:flex><span>Starting 1 process
</span></span><span style=display:flex><span>Jobs: 1 (f=1): [w(1)] [1.2% done] [0KB/2736KB/0KB /s] [0/684/0 iops] [eta 34m:53s]
</span></span></code></pre></div><h3 id=4k-read-iops>4K Read IOP/s</h3><p>A terrible average of 3017 IOP/s:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=32 --size=5G --readwrite=randread
</span></span><span style=display:flex><span>test: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32
</span></span><span style=display:flex><span>fio-2.1.11
</span></span><span style=display:flex><span>Starting 1 process
</span></span><span style=display:flex><span>Jobs: 1 (f=1): [r(1)] [5.6% done] [12068KB/0KB/0KB /s] [3017/0/0 iops] [eta 07m:20s]
</span></span></code></pre></div><h2 id=annoyances>Annoyances</h2><h3 id=open-files>Open Files</h3><p>Gluster seems to love open file handles, on an average node in the cluster with 120 small volumes connected to 3 fuse clients and no files I often see up to 1.5 <em>million</em> open files:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>root@int-gluster-02:~  # lsof | wc -l
</span></span><span style=display:flex><span>1042782
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@int-gluster-01:~  # netstat -lnp|grep gluster|wc -l
</span></span><span style=display:flex><span>111
</span></span></code></pre></div><h3 id=inconsistant-configuration>Inconsistant Configuration</h3><p>A big gripe I have is with the location of cluster and volume configuration, it is kept in a number of different files and file locations, for example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>/etc/glusterfs/glusterd.vol
</span></span><span style=display:flex><span>/var/lib/glusterd/options
</span></span><span style=display:flex><span>/var/lib/glusterd/glusterd.info
</span></span><span style=display:flex><span>/var/lib/glusterd/vols/simpleapp_static/quota.conf
</span></span><span style=display:flex><span>/var/lib/glusterd/vols/simpleapp_static/trusted-simpleapp_static.tcp-fuse.vol
</span></span><span style=display:flex><span>/var/lib/glusterd/vols/simpleapp_static/bricks/int-gluster-01\:-mnt-gluster-storage-simpleapp_static
</span></span></code></pre></div><p>This makes managing the cluster and volumes with automation tools a bit of a pain and thus most tooling calls out to the <code>gluster</code> command line application to set and read configuration which is not ideal.</p><h3 id=memory-usage--leaks>Memory Usage & Leaks</h3><p>Along the way with Gluster from version 3.10 to 3.12 I&rsquo;ve encountered <em>many</em> memory leaks resulting in OOMs, corrupt volume configuration and often entire cluster rebuilds.</p><p>A lot of these seem to occur when making a large number of changes to volume configuration.</p><p>Often when Gluster runs out of memory and gets OOM killed, it corrupts <code>.vol</code> or <code>.info</code> files in <code>/var/lib/glusterd/vols/&lt;volumes>/</code>, this causes the daemon to fail to start, requires you to hunt through logs for mentions of incorrectly configured volumes, delete the files in question and hope that they correctly sync back from another node, if they don&rsquo;t or can&rsquo;t be synced back it seems you might have to destroy the volumes (or cluster) and recreate them before restoring your (brick) data from backups - this is not at all fun.</p><h3 id=poor-gluster-cli-performance>Poor Gluster CLI Performance</h3><p>Again, because of the nature of cluter and volume configuration automation often has to call out to use the Gluster CLI tool to set or get information, this is made especially painful due to the tools performance - notibly how long it takes to set / get options on volumes.</p><p>Setting a volume option on average takes around 1 second and the tool cannot set an option on multiple volumes at once.</p><p>This quickly adds up, suppose you have 200 volumes and you&rsquo;re setting 5 options on each volume, <code>200*5=1000 / 60</code> - that&rsquo;s <em>16.6 minutes just to set the options!</em>, this could be a real issue when recovering from a disaster scenario.</p><h3 id=broken-links-to-glusterorg-and-glusterreadthedocsio>Broken Links to Gluster.org and Gluster.readthedocs.io</h3><p>Many times I&rsquo;ve clicked on links to Gluster articles or documentation and have found them to be broken, it seems that Gluster.org has undergone changes and has not created redirects for existing permalinks.</p></div></article><hr><p class=articleTagsContainer><span>ï€«</span>
<strong>Tags:</strong>
<a class=buttonTag href=../../../tags/software>#software</a>
<a class=buttonTag href=../../../tags/storage>#storage</a>
<a class=buttonTag href=../../../tags/tech>#tech</a></p><div class=relatedArticlesContainer><hr><h2>More posts like this</h2><div class=postlist><article class="card postlistitem"><div><h2><a href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/>Making Work Visible - Avoid DMs</a></h2><p class=date><span title=Date>ï—¬</span>
2022-10-20
|
<span title=Tags>ï€«</span>
<a href=../../../tags/communication>#communication</a>
<a href=../../../tags/culture>#culture</a>
<a href=../../../tags/tech>#tech</a>
<a href=../../../tags/work>#work</a></p><a class=unstyledLink href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/><img src=https://smcleod.net/2022/10/making-work-visible-avoid-dms//dm-disruption.jpg alt></a><div class=articlePreview><p><h3 id=avoid-sending-dmspms-for-support-and-technical-questions>Avoid sending DMs/PMs for support and technical questions</h3><p>This is a blurb I usually add to the wiki pages of teams I work with, it&rsquo;s a simple reminder that we should avoid sending direct messages (or worse - calling) individuals for support and technical questions.</p><p>Instead, we should use the appropriate channels for the team. This is a simple way to make work visible, share knowledge, reduce context switching, avoid silos and share the support load.</p></p><p><a href=https://smcleod.net/2022/10/making-work-visible-avoid-dms/>Continue reading ï•“</a></p></div></div><hr></article><article class="card postlistitem"><div><h2><a href=https://smcleod.net/2022/10/the-best-of-2022-edition/>The Best Of - 2022 Edition</a></h2><p class=date><span title=Date>ï—¬</span>
2022-10-19
|
<span title=Tags>ï€«</span>
<a href=../../../tags/bestof>#bestof</a>
<a href=../../../tags/software>#software</a></p><a class=unstyledLink href=https://smcleod.net/2022/10/the-best-of-2022-edition/><img src=https://smcleod.net/2022/10/the-best-of-2022-edition//tools.jpg alt></a><div class=articlePreview><p><p>Near the end of each year I note down a summary of the best apps I&rsquo;ve enjoyed using throughout the year, here&rsquo;s 2022.</p><p><em>See also <a href=https://smcleod.net/software/2022/05/16/firefox-addons-2022/>Firefox Addons - 2022</a></em></p><h2 id=the-best>The Best</h2><ul><li>MeetingBar (Meetings in your menubar) - <a href=https://github.com/leits/MeetingBar>Get MeetingBar</a></li><li>Reeder (RSS) - <a href=https://www.reederapp.com/>Get Reeder</a></li><li>Shottr (Better screenshots) - <a href=https://shottr.cc/>Get Shottr</a></li><li>Bumpr (Multi-browser rules for links) - <a href=https://www.getbumpr.com/>Get Bumper</a></li><li>Onyx (macOS customisation) - <a href=https://titanium-software.fr/en/onyx.html>Get OnyX</a></li><li>Brew (Package manager) - <a href=https://brew.sh/>Get Brew</a><ul><li>mas (Mac App Store manager for Brew) - <a href=https://github.com/mas-cli/mas>Get mas</a></li></ul></li><li>Little Snitch (Firewall) - <a href=https://obdev.at/products/littlesnitch>Get Little Snitch</a><ul><li>RadioSilence (Simple Firewall) - <a href=https://radiosilenceapp.com/>Get RadioSilence</a></li></ul></li><li>Pixelmator Pro (Image/Photo editor) - <a href=https://www.pixelmator.com/pro/>Get Pixelmator Pro</a></li><li>Keka (Archive/Compression tool) - <a href=https://www.keka.io/>Get Keka</a></li><li>LaunchControl (Frontend for launchd) - <a href=https://www.soma-zone.com/LaunchControl/>Get LaunchControl</a></li><li>Peek (Quicklook plugins) - <a href="https://apps.apple.com/au/app/peek-a-quick-look-extension/id1554235898?mt=12">Get Peek</a></li><li>MusicHarbor (Track new and upcoming music releases) - <a href=https://apps.apple.com/au/app/musicharbor-track-new-music/id1440405750>Get MusicHarbor</a></li><li>DaisyDisk (Disk usage inspector) - <a href=https://daisydiskapp.com/>Get DaisyDisk</a></li><li>Things (GTD/Notes/Lists app) - <a href=https://culturedcode.com/things/>Get Thing</a></li><li>BetterTouchTool (macOS automation and customisation) - <a href=https://folivora.ai/>Get BetterTouchTool</a></li><li>Parcel (Track parcels) - <a href=https://apps.apple.com/au/app/parcel/id375589283>Get Parcel</a></li><li>Backblaze (Backups) - <a href=https://www.backblaze.com/>Get Backblaze</a></li><li>MicroSnitch (Get notifications for active mic/webcam) - <a href=https://obdev.at/products/microsnitch/index.html>Get Micro Snitch</a></li><li>Handbrake (Transcode video files) - <a href=https://handbrake.fr/>Get Handbrake</a></li><li>Amphetamine (Keep you mac awake) - <a href="https://apps.apple.com/us/app/amphetamine/id937984704?mt=12">Get Amphetamine</a></li><li>Bear (Markdown/notes/web clipping) - <a href=https://bear.app/>Get Bear</a></li><li>Calibre (eBook management and conversation) - <a href=https://calibre-ebook.com/>Get Calibre</a></li><li>ImageOptim Beta (Image compression) - <a href=https://imageoptim.com/ImageOptim1.8.9a1.tar.bz2>Get ImageOptim</a><ul><li><em>Note: This is a beta version, but it&rsquo;s the only version that supports Apple Silicon natively, the author is working on <a href=https://github.com/ImageOptim/ImageOptim/issues/354>ImageOptim 2 - A complete rewrite</a></em>.</li></ul></li><li>Strongbox (Native password/secrets management) - <a href=https://strongboxsafe.com/>Get StrongBox</a><ul><li><em>Note: I will likely eventually replace 1Password with Strongbox, it&rsquo;s a native app (not Electron!), currently waiting on official browser extensions</em>.</li></ul></li><li>Stay (Window management) - <a href=https://cordlessdog.com/stay/>Get Stay</a></li><li>Firefox (Don&rsquo;t run a browser from an ad company) - <a href=https://www.mozilla.org/en-GB/firefox/new/>Get Firefox</a></li><li>Fastmail (Email) - <a href=https://www.fastmail.com>Get Fastmail</a></li><li>mu-repo (Run bulk commands across git repos) - <a href=https://fabioz.github.io/mu-repo/>Get mu-repo</a></li></ul><p>You can find all the applications I use, along with my shell config and plugins within the <a href=https://github.com/sammcj/zsh-bootstrap>BrewFile</a> which I use to bootstrap my macOS machines.</p><h2 id=the-worst>The Worst</h2><ul><li>ðŸ’© AWS CDK ðŸ’©</li><li>ðŸ’© 1Password 8 ðŸ’©</li><li>ðŸ’© Microsoft Teams ðŸ’©</li><li>ðŸ’© Microsoft Office365 ðŸ’©</li></ul></p><p><a href=https://smcleod.net/2022/10/the-best-of-2022-edition/>Continue reading ï•“</a></p></div></div><hr></article></div></div></main><footer><hr><div class=footerColumns><ul class=notlist><li><strong>Links</strong></li><li><a target=_blank href=https://github.com/sammcj>Github | @sammcj</a></li><li><a target=_blank href=https://twitter.com/s_mcleod>Twitter | @s_mcleod</a></li><li><a target=_blank href=https://smcleod.net>This Website | smcleod.net</a></li></ul></div><p><small>2022 &copy; Sam McLeod</small></p><p><small></small></p></footer></div></div></div></body></html>